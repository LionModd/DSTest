---
title: "Implementing Linear Regression"
author: "Dom"
date: 2018-06-29
output: 
  html_document:
    highlight: pygments
tags: [linear regression, tidyverse]
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
load(file = "C:/Users/dlelion/Desktop/Struggle/Testing/pasturesData.RData")
library(rmarkdown)
library(knitr)
```

## Packages used
```{r, echo = TRUE, warning = FALSE, message = FALSE}
library(tidyverse)
library(caTools)
```

I've found a dataset online to practice the method of **linear regression** on involving data regarding pasture rent in areas of Minnesota. The columns are

* Index
* Rent per arable acre ($)
* Milk cows per square mile
* Difference between pasture and arable land
* Rental price per grassy acre

with rental price per grassy acre being the **dependent variable**.
<p>
<p>
##Aim: find *one feature* that can be used to predict the dependent variable
<p>
First we'll do some preparation of the data, renaming of columns. This could have been dealt with in the source file of the data but where's the fun in that! I'm viewing this as a reinforement of my data prep skills.

```{r, echo = TRUE}
pastures %>%
  rename(index = Rental,
         rent.arable = price,
         cows = per,
         difference = grassy,
         rent.grassy = acre) -> pastures
pastures
```

Looking at each feature (except Index) against the dependent variable (**rent.grassy**) we can see a relationship with it and **rent.arable**. (see below)

```{r, echo = TRUE}
plot <- ggplot(pastures)

a <- plot + geom_point(aes(x = rent.arable, y = rent.grassy)) + ggtitle("Rent per arable acre ($)")
b <- plot + geom_point(aes(x = cows, y = rent.grassy)) + ggtitle ("Cows per sq mile")
c <- plot + geom_point(aes(x = difference, y = rent.grassy)) + ggtitle("Difference between pasture and arable land")

a
b
c
```

With this in mind we'll take this data off to one side to make things neater (using the trusty ``tidyverse``).

```{r, echo = TRUE}
pastures %>%
  select("rent.arable","rent.grassy") -> pastures.rent

pastures.rent
```

Now we'll split the data in a 1:3 ratio for testing and training data. This is important because we want to test our linear regression model on something, so why not use data we have!

```{r, echo = TRUE}
set.seed(345) # Set a random seed to allow the same split to be replicated

split <- sample.split(pastures.rent$rent.grassy, SplitRatio = 0.75) # Assign T/F to each row (100%)
train <- subset(pastures.rent, split == T)            # Pull out those that have T (75%)
test <- subset(pastures.rent, split == F)             # Pull out those that have F (25%)
```

Great now this is ready we can generate our regressor using the ``lm`` function. Here we're telling the function that **rent.grassy** is our dependent variable and **rent.arable** our independent variable and that we want to use our training dataset that we just produced.<p>

Let's generate this regressor as well as the correlation and see the information about them.

```{r, echo = TRUE}
cor(train$rent.arable, train$rent.grassy)

regressor <- lm(formula = rent.grassy ~ rent.arable,
                data = train)

summary(regressor)
```

The first value (the **correlation**) we're looking at if there is a correlation between the features. In this case 0.88 suggests that as x increases then y will. This makes sense from a visual point of view if you look at the first scatter plot. From this the regression line will most likely end up being a straight line from the lower left hand corner and up to the upper right. This was done to reinforce my understanding of correlation.

As we can see the **rent.arable** feature has a very **low p value** indicating that this is statistically significant i.e. **rent.arable** is a good predictor for **rent.grassy**. Ok it's the only independent variable used in this case and we saw that the original data has a somewhat straightness about it so I was expecting to see a low p value. <p>

Using this regressor we can now predict the test data's **rent.grassy**.


```{r, echo = TRUE}
ypred <- predict(regressor,test) # Predicting rent.grassy

ggplot()+
  geom_point(aes(x = train$rent.arable, y = train$rent.grassy)) +
  geom_point(aes(x = test$rent.arable, y = predict(regressor, test), color = "red")) +
  geom_line(aes(x = train$rent.arable, y = predict(regressor, train))) +
  ylab("rent.grassy") +
  xlab("rent.arable") +
  ggtitle("Red = predicted values") +
  theme(legend.position = "none")
```

This looks like the test data falls nicely amongst the data but the **rent.grassy** price is most likely not influenced by only **rent.arable** thus I may need to explore this dataset again but including a second maybe third feature to see if we can get a better prediction. This is a line of best fit currently.